from bs4 import BeautifulSoup
import urllib
import requests
import json
import csv

from dataClasses import User
from dataClasses import Business
from dataClasses import Review

if __name__ == '__main__':

    #this opens or creates author.csv , review.csv, restaurant.csv
    authors = open('user.csv', 'w')
    reviews = open('review.csv', 'w')
    restaurant = open('restaurant.csv', 'w')

    #Write header of restaurant
    restaurant.write('restaurantID,name,location,reviewCount,rating,categories,address,Hours,GoodforKids,AcceptsCreditCards,Parking,Attire,GoodforGroups,PriceRange,TakesReservations,Delivery,Takeout,WaiterService,OutdoorSeating,WiFi,GoodFor,Alcohol,NoiseLevel,Ambience,HasTV,Caters,WheelchairAccessible,webSite,phoneNumber,reviewCount \n')
    authors.write('authorID,name,location,reviewCount,friendCount,photoCount \n')

    #used to get inputs from attributes.json
    attributes_File = open('attributes.json', 'r').read()
    attributes = json.loads(attributes_File)

    #used to track index and if it should keep on trying to make calls to api.yelp
    offset = 0
    stillGettingFiles = True
    totalSize = 0

    #empty api format
    url = 'https://api.yelp.com/v3/businesses/search?offset={}&limit=50&categories={}&location={}'
    header = '"Authorization": "Bearer {}"'

    #filled api information to make a call
    urlNew = url.format(str(offset), attributes['categories'], attributes['location'])
    headerNew = '{' + header.format(attributes['api_key']) + '}'
    headerNew = json.loads(headerNew)

    websiteData = urllib.request.Request(urlNew, headers=headerNew, method='GET')
    websiteData = urllib.request.urlopen(websiteData).read().decode('utf8')

    listOfBusinesses = json.loads(websiteData)
    print (listOfBusinesses)
    totalSize = listOfBusinesses['total']
    print (totalSize)



    def website_parse(url):

        print (url)
        #Initialise dictionary to be returned
        data_dict = {}
        response_page = urllib.request.urlopen(url)
        html_page = response_page.read()
        soup = BeautifulSoup(html_page, 'html.parser')

        # Retrieve all other required info from 'More Business info'
        div = soup.find_all('div', {'class': 'ywidget'})

        for each_div in div:
            req_text = each_div.find(text='More business info')
            if req_text:

                #Retrieve the price range
                price = soup.find('dd', {'class': 'nowrap price-description'})
                if price:
                    data_dict['Price'] = price.text.strip()
                else:
                    data_dict['Price'] = ''

                #Retrieve the hours
                hours = soup.find('div', {'class': 'ywidget biz-hours'})
                if hours:
                    hours_table = hours.find('table', {'class': 'table table-simple hours-table'})
                    if hours_table:
                        hours_row = hours_table.find_all('tr')
                        if hours_row:
                            req_hours=''

                            for each in hours_row:
                                hours_day = each.find('th')
                                hours_time = each.find('td')
                                time = ''
                                each_seg = hours_time.find_all('span', {'class': 'nowrap'})
                                for each_part in each_seg:
                                    time = time + '-' + each_part.text.strip()
                                req_hours = req_hours + hours_day.text.strip() + time + ' '

                            data_dict['Hours'] = req_hours.strip()
                        else:
                            data_dict['Hours'] = ''
                    else:
                        data_dict['Hours'] = ''
                else:
                    data_dict['Hours'] = ''

                req_div = each_div
                req_data = req_div.find_all('dl')

                for each in req_data:
                    a = each.find('dt').text
                    b = each.find('dd').text

                    key = a.strip()

                    if key not in data_dict:
                        data_dict[key] = []
                    data_dict[key] = b.strip()

                # Retrieve author and review details
                review_div = soup.find('div', {'class': 'review-list'})
                review_li = review_div.find_all('li')

                for each_li in review_li[1:]:
                    userID_div = each_li.find_all('div', {'class': 'review review--with-sidebar'})
                    if (userID_div):
                        id = (userID_div)[0]['data-signup-object']
                        req_id = id[8:]
                        authors.write(req_id)
                        authors.write(',')

                        name = userID_div[0].find_all('li', {'class': 'user-name'})
                        name1 = name[0].find_all('a')
                        authors.write(name1[0].text.strip())
                        authors.write(',')

                        location = userID_div[0].find_all('li', {'class': 'user-location'})
                        if (location):
                            authors.write(location[0].text.strip().replace(',', ''))
                            authors.write(',')
                        else:
                            authors.write(',')

                        review_count = userID_div[0].find_all('li', {'class': 'review-count'})
                        if (review_count):
                            authors.write(review_count[0].text.strip()[0:-8])
                            authors.write(',')
                        else:
                            authors.write(',')

                        friend_count = userID_div[0].find_all('li', {'class': 'friend-count'})
                        if (friend_count):
                            authors.write(friend_count[0].text.strip()[0:-8])
                            authors.write(',')
                        else:
                            authors.write(',')

                        photo_count = userID_div[0].find_all('li', {'class': 'photo-count'})
                        if (photo_count):
                            authors.write(photo_count[0].text.strip()[0:-7])
                            authors.write('\n')
                        else:
                            authors.write('\n')

                # for num, each in enumerate(url):
                #     if each == '?':
                #         break;
                #
                # new_url = url[0:num + 1] + 'start=20'
                # response_page1 = urllib.request.urlopen(new_url)
                # html_page1 = response_page.read()
                # soup1 = BeautifulSoup(html_page1, 'html.parser')
                #
                # review_div = soup1.find('div', {'class': 'review-list'})
                # review_li = review_div.find_all('li')
                #
                # for each_li in review_li[1:]:
                #     userID_div = each_li.find_all('div', {'class': 'review review--with-sidebar'})
                #     if (userID_div):
                #         id = (userID_div)[0]['data-signup-object']
                #         req_id = id[8:]
                #         authors.write(req_id)
                #         authors.write(',')
                #
                #         name = userID_div[0].find_all('li', {'class': 'user-name'})
                #         name1 = name[0].find_all('a')
                #         authors.write(name1[0].text.strip())
                #         authors.write(',')
                #
                #         location = userID_div[0].find_all('li', {'class': 'user-location'})
                #         authors.write(location[0].text.strip().replace(',', ''))
                #         authors.write(',')
                #
                #         review_count = userID_div[0].find_all('li', {'class': 'review-count'})
                #         authors.write(review_count[0].text.strip()[0:-8])
                #         authors.write(',')
                #
                #         friend_count = userID_div[0].find_all('li', {'class': 'friend-count'})
                #         authors.write(friend_count[0].text.strip()[0:-8])
                #         authors.write(',')
                #
                #         photo_count = userID_div[0].find_all('li', {'class': 'photo-count'})
                #         authors.write(photo_count[0].text.strip()[0:-7])
                #         authors.write('\n')




                return data_dict
    count_valid=0
    count_invalid = 0
    #loops until we get all businesses
    while offset < totalSize:

        #do code here to check each business and update the csv files
        #we get 50 businesses so check array from 0 to 49
        for i in range(0,49):
            jsonBusiness = listOfBusinesses['businesses'][i]
            location = jsonBusiness['location']

            #check if business is within the zipcode we desire and has more then 20 reviews if so continue
            if location['zip_code'] == '60602' and jsonBusiness['review_count'] >= 20:
                count_valid = count_valid+1
                website_data = website_parse(jsonBusiness['url'])
                #print (website_data)
                if website_data:
                    #Retrieve from the yelp API
                    Business.businessID = jsonBusiness['id']
                    Business.name = jsonBusiness['name']

                    #Convert location from dict to string
                    a = []
                    value = ''
                    dict = jsonBusiness['location']
                    for key in dict:
                        a.append(dict[key])
                    a = a[0:-1]

                    for each in a:
                        if each:
                           value = value + each + " "

                    Business.location = value.strip()    #concatenate or display as such?
                    Business.webSite = jsonBusiness['url']
                    Business.reviewCount = str(jsonBusiness['review_count'])
                    Business.rating = str(jsonBusiness['rating'])

                    #Convert categories from list to single string
                    a = jsonBusiness['categories']
                    text = ''
                    for each in a:
                        for key in each:
                            text = text + key + ':' + each[key] + ' '

                    Business.categories = text.strip() #concatenate or display as such?

                    a = location['display_address']
                    text = ''
                    for each in a:
                        text = text + each + ' '

                    Business.address = text.strip() #concatenate or display as such ? Same as location?
                    Business.phoneNumber = str(jsonBusiness['phone'])

                    #Retrieve from website
                    Business.Hours = website_data.get('Hours','')
                    Business.GoodforKids = website_data.get('Good for Kids','')
                    Business.AcceptsCreditCards = website_data.get('Accepts Credit Cards','')
                    Business.Parking = website_data.get('Parking','')
                    Business.Attire = website_data.get('Attire','')
                    Business.GoodforGroups = website_data.get('Good for Groups','')
                    Business.PriceRange = website_data.get('Price','')
                    Business.TakesReservations = website_data.get('Takes Reservations','')
                    Business.Delivery = website_data.get('Delivery','')
                    Business.Takeout = website_data.get('Take-out','')
                    Business.WaiterService = website_data.get('Waiter Service','')
                    Business.OutdoorSeating = website_data.get('Outdoor Seating','')
                    Business.WiFi = website_data.get('Wi-Fi','')
                    Business.GoodFor = website_data.get('Good For','')
                    Business.Alcohol = website_data.get('Alcohol','')
                    Business.NoiseLevel = website_data.get('Noise Level','')
                    Business.Ambience = website_data.get('Ambience','')
                    Business.HasTV = website_data.get('Has TV','')
                    Business.Caters = website_data.get('Caters','')
                    Business.WheelchairAccessible = website_data.get('Wheelchair Accessible','')

                    data = [Business.businessID,Business.name,Business.location,Business.reviewCount,Business.rating,Business.categories,Business.address,Business.Hours,Business.GoodforKids,Business.AcceptsCreditCards,Business.Parking,Business.Attire,Business.GoodforGroups,Business.PriceRange,Business.TakesReservations,Business.Delivery,Business.Takeout,Business.WaiterService,Business.OutdoorSeating,Business.WiFi,Business.GoodFor,Business.Alcohol,Business.NoiseLevel,Business.Ambience,Business.HasTV,Business.Caters,Business.WheelchairAccessible,Business.webSite,Business.phoneNumber,Business.reviewCount]
                    #print (data)
                    for count,each in enumerate(data):
                        if count != len(data)-1:
                            restaurant.write(each)
                            restaurant.write(',')
                        else:
                            restaurant.write(each)
                    restaurant.write('\n')
            count_invalid = count_invalid +1
            '''
            if in zipcode:
                #do code to extract business info from yelp api to Business.class
                #do code to fetch yelp business page
                #do code to view reviews and parse through array of reviews and add to review csv
                        #if user is not in user.csv:
                            #get user data
            '''




        #increase offset
        offset += 50

        #get next set of businesses
        urlNew = url.format(str(offset), attributes['categories'], attributes['location'])
        websiteData = urllib.request.Request(urlNew, headers=headerNew, method='GET')
        websiteData = urllib.request.urlopen(websiteData).read().decode('utf8')
        listOfBusinesses = json.loads(websiteData)
        print(listOfBusinesses)
        print ("count_valid" + str(count_valid))
        print ("count_invalid" + str(count_invalid))
